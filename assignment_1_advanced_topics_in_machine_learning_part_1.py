# -*- coding: utf-8 -*-
"""Assignment_1_Advanced_Topics_in_Machine_Learning_Part_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C31lR0gcbUmlmFVvaNG8nYgqtRSsyxL_

Credit Risk Assignment

The assignment required 3 techniques of sampling, weighting and the expected cost minimization for these 3 different algorithm (random forest, Linear SVM and gaussian naïve bayes)

The method are applied:

1. w/o any technique the random forest, linear SVM and gaussian naïve bayes algorithms, provide the peformance (plot, confusion matrix and the score from the total cost)
2. sampling method: 
      (i) oversampling (same metrics)
      (ii) undersampling (same metrics)
3. weighting method (same metrics)
4. expected cost minimization (same metrics)
"""

# import the libraries
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from collections import Counter
from sklearn.datasets import  fetch_openml
from sklearn.compose import make_column_transformer, make_column_selector
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
import pandas as pd
import numpy as np
from sklearn.metrics import f1_score, accuracy_score
import matplotlib.pyplot as plt

#load the dataset

X, y = fetch_openml("credit-g", version=1, as_frame=True, parser='auto', return_X_y=True)

# check feautre names
Feature_names = X.columns
print(Feature_names)
#print(X.head)

# tranform the data for the algorithms using the OneHotEncoder and scaler

# Select categorical columns for encoding
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()

# Select numerical columns for scaling
numerical_cols = X.select_dtypes(include=['int', 'float']).columns.tolist()

# Encode the categorical columns
encoder = OneHotEncoder()
encoded_array = encoder.fit_transform(X[categorical_cols]).toarray()
encoded_df = pd.DataFrame(encoded_array, columns=[f"{col}_{val}" for col, vals in zip(categorical_cols, encoder.categories_) for val in vals])


# Scale the numerical columns
scaler = StandardScaler()
scaled_array = scaler.fit_transform(X[numerical_cols])
scaled_df = pd.DataFrame(scaled_array, columns=numerical_cols)

# Combine the encoded and scaled DataFrames with the original DataFrame
X = pd.concat([X.drop(categorical_cols + numerical_cols, axis=1), encoded_df, scaled_df], axis=1)

# Print the resulting DataFrame
#print(X.head())


#check the data
#print(X.head())

# check the good and bad cases
print(Counter(y))

"""# **The first approach is w/o any technique**


"""

#split the data to train and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# create a cost-matrix as in slides
cost_m = [[0, 1], 
          [4, 0]]

#algorithms names 
names = ['random forest', 'linear SVM', 'Gaussian Naïve Bayes']

# we dont tune the model and we used fix values for the hyperparameters
classifiers = [RandomForestClassifier(n_estimators=100, random_state=42), 
               SVC(kernel='linear'), GaussianNB()]

#apply the algorithms random forest, linear SVM and gaussian naïve bayes and their metrics and their cost matrix
# Train and test the classifiers
accuracies = []
cost_sensitivity = []
cost_sensitivity_all = []

for name, clf in zip(names, classifiers):
  print(" ")
  print(name)
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)
  print(classification_report(y_test, y_pred, target_names=['bad', 'good']))
  conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides
  acc = accuracy_score(y_test, y_pred)
  accuracies.append(acc)
  cost = np.sum(conf_m * cost_m)
  cost_sensitivity.append(cost)
  cost_sensitivity_all.append(cost) 
  print(conf_m) 
  print("Total Cost: ", np.sum(conf_m * cost_m))


#gaussian naïve bayes peform better based on the score of the total cost

#Accuracy Metric (similar approach for other metrics such as F1, Recall, precision etc)
# Plot the accuracies for each algorithm
fig, ax = plt.subplots()
x = np.arange(len(names))
width = 0.4

rects1 = ax.bar(x, accuracies, width)

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Accuracy')
ax.set_title('Accuracy for each algorithm')
ax.set_xticks(x)
ax.set_xticklabels(names)

plt.show()

"""**The main metric for the algorithms peformance will use the score from the Total Cost** """

# Plot the Total Cost for each algorithm
fig, ax = plt.subplots()
x = np.arange(len(names))
width = 0.4

rects1 = ax.bar(x, cost_sensitivity, width)

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Cost for each algorithm')
ax.set_title('Cost sensitivity Analysis')
ax.set_xticks(x)
ax.set_xticklabels(names)

plt.show()

"""The order from the best to the worst peformance algorith w/o any technique is  as follow: Gaussian NB > Linear SVM > Random Forest.

# **Sampling method (oversampling and undersampling)**
"""

#The training set include: good cases are 491 and bad cases are 209
print("Counter before oversampling: ", Counter(y_train))


#The first sampling method is to oversample the data with bad cases that support the increasing of the total cost in the case of wrong prediction
#create a new dataset with oversampling
sampler = RandomOverSampler(sampling_strategy={'good':491 , 'bad': 491}, random_state=42) 
X_rs, y_rs = sampler.fit_resample(X_train, y_train)
print("Counter after oversampling:",Counter(y_rs))


#apply the algorithms random forest, linear SVM and gaussian naïve bayes and their metrics and their cost matrix
# Train and test the classifiers
accuracies1 = []
cost_sensitivity1 = []

for name, clf in zip(names, classifiers):
  print(" ")
  print(name)
  clf.fit(X_rs, y_rs)
  y_pred = clf.predict(X_test)
  print(classification_report(y_test, y_pred, target_names=['bad', 'good']))
  conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides
  acc = accuracy_score(y_test, y_pred)
  accuracies1.append(acc)
  cost = np.sum(conf_m * cost_m)
  cost_sensitivity1.append(cost)
  cost_sensitivity_all.append(cost) 
  print(conf_m) 
  print("Total Cost: ", np.sum(conf_m * cost_m))

names_oversampling = [name + '_oversampling' for name in names]
names_end =  names + names_oversampling
#print(names_end)
#gaussian naïve bayes peform better based on the score of the total cost

# Plot the Total Cost for each algorithm
fig, ax = plt.subplots()
x = np.arange(len(names))
width = 0.4

rects1 = ax.bar(x, cost_sensitivity1, width)

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Cost for each algorithm with oversampling')
ax.set_title('Cost sensitivity Analysis')
ax.set_xticks(x)
ax.set_xticklabels(names)

plt.show()

#The Gaussian NB is the best method based on the Total cost peformance

#The second sampling method is to undersample the data with good cases
#create a new dataset with undersampling

#training the data
sampler = RandomUnderSampler(sampling_strategy={'good':209 , 'bad': 209}, random_state=42) 
X_rs, y_rs = sampler.fit_resample(X_train, y_train)
print("Counter after undersampling:",Counter(y_rs))


#apply the algorithms random forest, linear SVM and gaussian naïve bayes and their metrics and their cost matrix
# Train and test the classifiers
cost_sensitivity2 = []

for name, clf in zip(names, classifiers):
  print(" ")
  print(name)
  clf.fit(X_rs, y_rs)
  y_pred = clf.predict(X_test)
  print(classification_report(y_test, y_pred, target_names=['bad', 'good']))
  conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides
  cost = np.sum(conf_m * cost_m)
  cost_sensitivity2.append(cost)
  cost_sensitivity_all.append(cost) 
  print(conf_m) 
  print("Total Cost: ", np.sum(conf_m * cost_m))

names_undersampling = [name + '_undersampling' for name in names]
names_end =  names_end + names_undersampling
#print(names_end)
#gaussian naïve bayes peform better based on the score of the total cost

# Plot the Total Cost for each algorithm
fig, ax = plt.subplots()
x = np.arange(len(names))
width = 0.4

rects1 = ax.bar(x, cost_sensitivity2, width)

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Cost for each algorithm with undersampling')
ax.set_title('Cost sensitivity Analysis')
ax.set_xticks(x)
ax.set_xticklabels(names)

plt.show()

#The Gaussian NB is the best method based on the Total cost peformance, but we observed a different order on the peformance in the other two algorithms. The rf starts to peform better than the SVM.

"""# **Weighting Method**"""

#create the weights
weights = np.zeros(y_train.shape[0])
weights[np.where(y_train == 'good')] = 1;
weights[np.where(y_train == 'bad')] = 4;


#apply the algorithms random forest, linear SVM and gaussian naïve bayes and their metrics and their cost matrix
# Train and test the classifiers
cost_sensitivity3 = []

for name, clf in zip(names, classifiers):
  print(" ")
  print(name)
  clf.fit(X_train, y_train, weights)
  y_pred = clf.predict(X_test)
  print(classification_report(y_test, y_pred, target_names=['bad', 'good']))
  conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides
  cost = np.sum(conf_m * cost_m)
  cost_sensitivity3.append(cost)
  cost_sensitivity_all.append(cost) 
  print(conf_m) 
  print("Total Cost: ", np.sum(conf_m * cost_m))


names_weighted = [name + '_weighted' for name in names]
names_end =  names_end + names_weighted

#The weighted method shows significant changes on the peformance. The linear SVM peform better than the other algorithms. The order for the peformance based on the total cost is SVM > Gaussian NB > RF.

# Plot the Total Cost for each algorithm
fig, ax = plt.subplots()
x = np.arange(len(names))
width = 0.4

rects1 = ax.bar(x, cost_sensitivity3, width)

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Cost for each algorithm with weighted method')
ax.set_title('Cost sensitivity Analysis')
ax.set_xticks(x)
ax.set_xticklabels(names)

plt.show()

"""# **Minimizing expected cost**"""

#first we will minimize it without probability calibration
#we need to have numerical values in our target column to perform the matrix multiplication
#and calculate the probabilities.

label_map = {"good": 0, "bad": 1}

y_train_num = [label_map[c] for c in y_train]
y_test_num = [label_map[c] for c in y_test]

#apply the algorithms random forest, linear SVM and gaussian naïve bayes and their metrics and their cost matrix
# Train and test the classifiers
cost_sensitivity4 = []

classifiers = [RandomForestClassifier(n_estimators=100, random_state=42), 
               SVC(kernel='linear', probability=True), GaussianNB()]

for name, clf in zip(names, classifiers):
  print(" ")
  print(name)
  model = clf.fit(X_train, y_train_num)
  y_pred_prob = model.predict_proba(X_test)
  y_pred = np.argmin(np.matmul(y_pred_prob, np.array(cost_m).T), axis=1) 
  print(classification_report(y_test_num, y_pred, target_names=['bad', 'good']))
  conf_m = confusion_matrix(y_test_num, y_pred).T
  cost = np.sum(conf_m * cost_m)
  cost_sensitivity4.append(cost)
  cost_sensitivity_all.append(cost) 
  print(conf_m) 
  print("Total Cost: ", np.sum(conf_m * cost_m))


names_minexpcost = [name + '_minexpcost' for name in names]
names_end =  names_end + names_minexpcost

#The SVM is slight better than the rf and for fist time the Guassian is the worst algorithm. In this method the Gaussian increase significant the total cost.

# Plot the Total Cost for each algorithm
fig, ax = plt.subplots()
x = np.arange(len(names))
width = 0.4

rects1 = ax.bar(x, cost_sensitivity4, width)

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Cost for each algorithm with Minimizing expected cost')
ax.set_title('Cost sensitivity Analysis')
ax.set_xticks(x)
ax.set_xticklabels(names)

plt.show()

"""# **Plots the peformance for every algo and method**"""

# Plot the Total Cost for each algorithm for each method
fig, ax = plt.subplots()
x = np.arange(len(names_end))
width = 0.4

rects1 = ax.bar(x, cost_sensitivity_all, width)

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Cost for each algorithm per method')
ax.set_title('Cost sensitivity Analysis')
ax.set_xticks(x)
ax.set_xticklabels(names_end, rotation=45, ha='right')

plt.show()

"""The empirical results indicate that the algorithm's performance varies depending on the method used. For oversampling and undersampling methods, the recommended algorithm is Gaussian, while for weighted and minimize exponential cost methods, the recommended algorithm is Linear SVM. """
